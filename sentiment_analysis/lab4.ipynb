{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"lab4.ipynb","provenance":[],"collapsed_sections":["NiBiwWIA9EDg","01EZrfCD9ED0","95BWIa4k9ED4","y1ZpKrzh9ED5"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"gx2aPodr9EDc"},"source":["# TP : Sentiment analysis on IMDB movie reviews"],"id":"gx2aPodr9EDc"},{"cell_type":"markdown","metadata":{"id":"NiBiwWIA9EDg"},"source":["## Objectives:\n","\n","1. Use a simple way to represent textual data - Bag of words\n","2. Test it with a simple model for a sentiment classification task - Naïve Bayesian\n","3. Explore improvements to these simple representations\n","4. Explore various methods to obtain dense representations (word embeddings) of the same data\n","5. Test these new representations with a simple classification model\n","6. Fine-tune a BERT model to execute the same classification task"],"id":"NiBiwWIA9EDg"},{"cell_type":"markdown","metadata":{"id":"phwroSJp9EDi"},"source":["## Necessary dependencies\n","\n","We will need the following packages:\n","- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n","- The Natural Language Toolkit : http://www.nltk.org/install.html\n","\n","Both are available with Anaconda: https://anaconda.org/anaconda/nltk and https://anaconda.org/anaconda/scikit-learn"],"id":"phwroSJp9EDi"},{"cell_type":"code","metadata":{"id":"ZiQKShse9EDj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970326551,"user_tz":-60,"elapsed":6973,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"0499a3a7-40eb-4575-a8c0-19211a4b6f72"},"source":["import os.path as op\n","import re \n","import numpy as np\n","import matplotlib.pyplot as plt\n","# !pip install gensim==4.1.2 # install the latest version if errors arise with gensim"],"id":"ZiQKShse9EDj","execution_count":162,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim==4.1.2 in /usr/local/lib/python3.7/dist-packages (4.1.2)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.4.1)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (1.19.5)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.1.2) (5.2.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"_n1jNo9m9EDk"},"source":["## Loading data\n","\n","We retrieve the textual data in the variable *texts*.\n","\n","The labels are retrieved in the variable $y$ - it contains *len(texts)* of them: $0$ indicates that the corresponding review is negative while $1$ indicates that it is positive."],"id":"_n1jNo9m9EDk"},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"pFIqfbu99EDl","executionInfo":{"status":"ok","timestamp":1635970345739,"user_tz":-60,"elapsed":16394,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"c4c243f2-5f0d-4b45-b0e1-4aa1d8656511"},"source":["# For those on google colab: you can download the files directly with this:\n","import gdown\n","gdown.download(\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", output=\"aclImdb_v1.tar.gz\", quiet=False)\n","!tar xzf /content/aclImdb_v1.tar.gz"],"id":"pFIqfbu99EDl","execution_count":163,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","To: /content/aclImdb_v1.tar.gz\n","100%|██████████| 84.1M/84.1M [00:01<00:00, 43.2MB/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"VfGmTOGY9EDm","executionInfo":{"status":"ok","timestamp":1635970346965,"user_tz":-60,"elapsed":1261,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"c81aa9b4-67bd-4bc7-eada-9bf478adac22"},"source":["from glob import glob\n","# We get the files from the path: ./aclImdb/train/neg for negative reviews, and ./aclImdb/train/pos for positive reviews\n","train_filenames_neg = sorted(glob(op.join('.', 'aclImdb', 'train', 'neg', '*.txt')))\n","train_filenames_pos = sorted(glob(op.join('.', 'aclImdb', 'train', 'pos', '*.txt')))\n","\n","\"\"\"\n","test_filenames_neg = sorted(glob(op.join('.', 'aclImdb', 'test', 'neg', '*.txt')))\n","test_filenames_pos = sorted(glob(op.join('.', 'aclImdb', 'test', 'pos', '*.txt')))\n","\"\"\"\n","\n","# Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n","train_texts_neg = [open(f, encoding=\"utf8\").read() for f in train_filenames_neg]\n","train_texts_pos = [open(f, encoding=\"utf8\").read() for f in train_filenames_pos]\n","train_texts = train_texts_neg + train_texts_pos\n","\n","\"\"\"\n","test_texts_neg = [open(f, encoding=\"utf8\").read() for f in test_filenames_neg]\n","test_texts_pos = [open(f, encoding=\"utf8\").read() for f in test_filenames_pos]\n","test_texts = test_texts_neg + test_texts_pos\n","\"\"\"\n","\n","# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n","# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n","train_labels = np.ones(len(train_texts), dtype=int)\n","train_labels[:len(train_texts_neg)] = 0.\n","\n","\"\"\"\n","test_labels = np.ones(len(test_texts), dtype=np.int)\n","test_labels[:len(test_texts_neg)] = 0.\n","\"\"\""],"id":"VfGmTOGY9EDm","execution_count":164,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ntest_labels = np.ones(len(test_texts), dtype=np.int)\\ntest_labels[:len(test_texts_neg)] = 0.\\n'"]},"metadata":{},"execution_count":164}]},{"cell_type":"markdown","metadata":{"id":"Iomn7UxK9EDn"},"source":["Example of one document:"],"id":"Iomn7UxK9EDn"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"pNSiDVn09EDp","executionInfo":{"status":"ok","timestamp":1635970346966,"user_tz":-60,"elapsed":41,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"6ebaae00-e26b-412e-eabf-7388c7cadda4"},"source":["open(\"./aclImdb/train/neg/0_3.txt\", encoding=\"utf8\").read()"],"id":"pNSiDVn09EDp","execution_count":165,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\""]},"metadata":{},"execution_count":165}]},{"cell_type":"markdown","metadata":{"id":"sEdOAmqN9EDq"},"source":["**In this lab, the impact of our choice of representations upon our results will also depend on the quantity of data we use:** try to see how changing the parameter ```k``` affects our results !"],"id":"sEdOAmqN9EDq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdxtNGZp9EDq","executionInfo":{"status":"ok","timestamp":1635970346967,"user_tz":-60,"elapsed":37,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"ced657b9-e5c0-464e-d017-5e30e174deb5"},"source":["# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n","# Use an even number to keep the same number of positive and negative reviews\n","k = 100\n","train_texts_reduced = train_texts[0::k]\n","train_labels_reduced = train_labels[0::k]\n","\n","print('Number of documents:', len(train_texts_reduced))"],"id":"NdxtNGZp9EDq","execution_count":166,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of documents: 250\n"]}]},{"cell_type":"markdown","metadata":{"id":"vsGl4o4t9EDr"},"source":["We can use a function from sklearn, ```train_test_split```, to separate data into training and validation sets:"],"id":"vsGl4o4t9EDr"},{"cell_type":"code","metadata":{"id":"ThyBgsf19EDr","executionInfo":{"status":"ok","timestamp":1635970346967,"user_tz":-60,"elapsed":25,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from sklearn.model_selection import train_test_split"],"id":"ThyBgsf19EDr","execution_count":167,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_YAUkas9EDs","executionInfo":{"status":"ok","timestamp":1635970346968,"user_tz":-60,"elapsed":25,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["train_texts_splt, val_texts, train_labels_splt, val_labels = train_test_split(train_texts_reduced, train_labels_reduced, test_size=.2)"],"id":"g_YAUkas9EDs","execution_count":168,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w_c-laZX9EDs"},"source":["## Adapted representation of documents\n","\n","Our statistical model, like most models applied to textual data, uses counts of word occurrences in a document. Thus, a very convenient way to represent a document is to use a Bag-of-Words (BoW) vector, containing the counts of each word (regardless of their order of occurrence) in the document. \n","\n","If we consider the set of all the words appearing in our $T$ training documents, which we note $V$ (Vocabulary), we can create **an index**, which is a bijection associating to each $w$ word an integer, which will be its position in $V$. \n","\n","Thus, for a document extracted from a set of documents containing $|V|$ different words, a BoW representation will be a vector of size $|V|$, whose value at the index of a word $w$ will be its number of occurrences in the document. \n","\n","We can use the **CountVectorizer** class from scikit-learn to obtain these representations:"],"id":"w_c-laZX9EDs"},{"cell_type":"code","metadata":{"id":"Ozqpm9on9EDs","executionInfo":{"status":"ok","timestamp":1635970346969,"user_tz":-60,"elapsed":25,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from sklearn.feature_extraction.text import CountVectorizer"],"id":"Ozqpm9on9EDs","execution_count":169,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ACT_y7z9EDt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970346969,"user_tz":-60,"elapsed":24,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"f520df1e-6583-4c55-e124-665b8f135e13"},"source":["corpus = ['I walked down down the boulevard',\n","          'I walked down the avenue',\n","          'I ran down the boulevard',\n","          'I walk down the city',\n","          'I walk down the the avenue']\n","vectorizer = CountVectorizer()\n","\n","Bow = vectorizer.fit_transform(corpus)\n","\n","print(vectorizer.get_feature_names())\n","Bow.toarray()"],"id":"3ACT_y7z9EDt","execution_count":170,"outputs":[{"output_type":"stream","name":"stdout","text":["['avenue', 'boulevard', 'city', 'down', 'ran', 'the', 'walk', 'walked']\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 0, 2, 0, 1, 0, 1],\n","       [1, 0, 0, 1, 0, 1, 0, 1],\n","       [0, 1, 0, 1, 1, 1, 0, 0],\n","       [0, 0, 1, 1, 0, 1, 1, 0],\n","       [1, 0, 0, 1, 0, 2, 1, 0]])"]},"metadata":{},"execution_count":170}]},{"cell_type":"markdown","metadata":{"id":"ewWoz0uo9EDt"},"source":["Careful: check the memory that the representations are going to use (given the way they are build). What ```CountVectorizer``` argument allows to avoid the issue ? <br>\n","**Answer**: For each document in the corpus, we take into account the number of occurences of <u>all</u> unique words in the corpus which results in the length of $|V|$ (for each document). However, some words for a given document may occur less or even, have no frequency. Thus, to reduce the length of this representation, one can benefit from the *max_features* parameter of CountVectorizer, which allows us to keep only the top max_features words based on their frequency. Consequently, this results in the reduction of the lenght for the document representation.\n"],"id":"ewWoz0uo9EDt"},{"cell_type":"code","metadata":{"id":"VvSuovXq9EDu","executionInfo":{"status":"ok","timestamp":1635970347281,"user_tz":-60,"elapsed":327,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["# Create and fit the vectorizer to the training data\n","vectorizer = CountVectorizer()\n","Bow = vectorizer.fit_transform(train_texts_splt)"],"id":"VvSuovXq9EDu","execution_count":171,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXk7YHRD9EDu","executionInfo":{"status":"ok","timestamp":1635970347281,"user_tz":-60,"elapsed":13,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["# Transform the validation data\n","val_texts_transformed = vectorizer.transform(val_texts)"],"id":"sXk7YHRD9EDu","execution_count":172,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lh0mE8Q9EDv"},"source":["We are going to use the scikit-learn ```MultinomialNB```, an implementation of the Naive Bayesian model. Here, what is the naïve hypothesis ? <br>\n","**Answer**: In this scenario, naïveness is related to the fact that the probability of occurence for each word (feature) within a document (or in different documents) is independent."],"id":"8lh0mE8Q9EDv"},{"cell_type":"code","metadata":{"id":"_rPylnog9EDv","executionInfo":{"status":"ok","timestamp":1635970347282,"user_tz":-60,"elapsed":14,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from sklearn.naive_bayes import MultinomialNB"],"id":"_rPylnog9EDv","execution_count":173,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSvXAklv9EDv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970347283,"user_tz":-60,"elapsed":14,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"93c3d8b2-ae0f-4e66-a2ba-32e70245f9a7"},"source":["# Fit the model on the training data\n","\n","multinomial_nb = MultinomialNB()\n","multinomial_nb.fit(Bow, train_labels_splt)"],"id":"lSvXAklv9EDv","execution_count":174,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{},"execution_count":174}]},{"cell_type":"code","metadata":{"id":"4JFJsQu79EDw","executionInfo":{"status":"ok","timestamp":1635970347283,"user_tz":-60,"elapsed":10,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"],"id":"4JFJsQu79EDw","execution_count":175,"outputs":[]},{"cell_type":"code","metadata":{"id":"zHNRy5KU9EDw","colab":{"base_uri":"https://localhost:8080/","height":432},"executionInfo":{"status":"ok","timestamp":1635970347490,"user_tz":-60,"elapsed":216,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"5af3f67b-556a-4173-df75-c1bf74ed57c4"},"source":["# Test it on the validation data\n","y_pred = multinomial_nb.predict(val_texts_transformed)\n","cm = confusion_matrix(val_labels, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=multinomial_nb.classes_)\n","disp.plot()\n","plt.show()\n","print(classification_report(val_labels, y_pred))"],"id":"zHNRy5KU9EDw","execution_count":176,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.72      0.85      0.78        27\n","           1       0.78      0.61      0.68        23\n","\n","    accuracy                           0.74        50\n","   macro avg       0.75      0.73      0.73        50\n","weighted avg       0.75      0.74      0.74        50\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"bMHSjK6x9EDw"},"source":["Let us look at the *features* built by the ```vectorizer```. How can we improve them ? <br>\n","**Answer**: After printing some features below, we may see numbers with some letters following them, causing redundancy in some sense (for example, '70' and '70th' point to the same meaning of 70). We can apply some sort of stemming to remove the ending letters, which may result in a simpler representation. This can be applied not only on numbers, but also on words (for ex, abandoned --> abandon)."],"id":"bMHSjK6x9EDw"},{"cell_type":"code","metadata":{"id":"9NQWEz2f9EDw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635974567143,"user_tz":-60,"elapsed":18,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"85d877b4-b740-490b-8941-8c8d866ebe0b"},"source":["print(vectorizer.get_feature_names()[:100])"],"id":"9NQWEz2f9EDw","execution_count":301,"outputs":[{"output_type":"stream","name":"stdout","text":["['01', '10', '100', '11', '12', '13', '14', '15', '16', '17', '180', '1840', '1840s', '1920', '1930s', '1934', '1940s', '1944', '1950', '1953', '1954', '1959', '1960s', '1964', '1965', '1968', '1969', '1970', '1970s', '1971', '1973', '1974', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1989', '1992', '1993', '1994', '1995', '1996', '1997', '1999', '19th', '20', '2000', '2004', '2005', '2006', '2031', '22', '24', '25', '250', '269', '28', '2nd', '30', '35', '35mm', '36', '37', '3d', '3rd', '40', '44', '45', '4m', '4th', '4w', '50', '5000', '50s', '5th', '60', '65', '70', '70mm', '70s', '70th', '74', '750', '80', '80s', '90', '94', '99', '998', 'aaaand', 'abandoned', 'abc', 'ability', 'able', 'ably']\n"]}]},{"cell_type":"markdown","metadata":{"id":"7461FbBD9EDx"},"source":["### Improving representations\n","\n","Mainly, the arguments of the class ```vectorizer``` will allow us to easily change the way our textual data is represented. Let us try to work on our *Bag-of-words* representations:\n","   \n","#### Do not take into account words that are too frequent:\n","\n","You can use the argument ```max_df=1.0``` to change the amount of words taken into account. \n","\n","#### Try different granularities:\n","\n","Rather than just counting words, we can count sequences of words - limited in size, of course. \n","We call a sequence of $n$ words a $n$-gram: let's try using 2 and 3-grams (bi- and trigrams).\n","We can also try to use character sequences instead of word sequences.\n","\n","We will be interested in the options ```analyze='word'``` and ```ngram_range=(1, 2)``` which we'll change to alter the granularity. \n","\n","**Again: using these ways of getting more features from our text will probably have more impact if we do not have much training data to begin with !**\n","\n","To accelerate experiments, use the ```Pipeline``` tool from scikit-learn. "],"id":"7461FbBD9EDx"},{"cell_type":"code","metadata":{"id":"p7MqTfNT9EDx","executionInfo":{"status":"ok","timestamp":1635970355271,"user_tz":-60,"elapsed":3,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from sklearn.pipeline import Pipeline"],"id":"p7MqTfNT9EDx","execution_count":178,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXT_5if-9EDx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970355518,"user_tz":-60,"elapsed":16,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"c5b0ad1c-f6fa-47a4-9ea0-678b0ad64b75"},"source":["pipeline_base = Pipeline([\n","    ('vect', CountVectorizer(max_features=30000, analyzer='word', stop_words=None)),\n","    ('clf', MultinomialNB()),\n","])\n","# Fit and test the pipeline\n","pipeline_base.fit(train_texts_splt, train_labels_splt)\n","print('Score:', pipeline_base.score(val_texts, val_labels))"],"id":"rXT_5if-9EDx","execution_count":179,"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.74\n"]}]},{"cell_type":"code","metadata":{"id":"5tvMgE9l9EDy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970356817,"user_tz":-60,"elapsed":190,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"68daf082-8dbd-4a7e-a983-aeaf60302b75"},"source":["# Fit and test a pipeline with bigrams\n","pipeline_bigram = Pipeline([\n","    ('vect', CountVectorizer(max_features=30000, analyzer='word', stop_words=None, ngram_range = (2, 2))),\n","    ('clf', MultinomialNB()),\n","])\n","pipeline_bigram.fit(train_texts_splt, train_labels_splt)\n","print('Score:', pipeline_bigram.score(val_texts, val_labels))"],"id":"5tvMgE9l9EDy","execution_count":180,"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.66\n"]}]},{"cell_type":"code","metadata":{"id":"Z52QeHf-9EDy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970358176,"user_tz":-60,"elapsed":312,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"0f071ed1-7df4-4e9a-8696-0dffc6d380a3"},"source":["# ... trigrams\n","pipeline_trigram = Pipeline([\n","    ('vect', CountVectorizer(max_features=30000, analyzer='word', stop_words=None, ngram_range = (3, 3))),\n","    ('clf', MultinomialNB()),\n","])\n","pipeline_trigram.fit(train_texts_splt, train_labels_splt)\n","print('Score:', pipeline_trigram.score(val_texts, val_labels))"],"id":"Z52QeHf-9EDy","execution_count":181,"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.72\n"]}]},{"cell_type":"code","metadata":{"id":"Bwbzp_pg9EDy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970359081,"user_tz":-60,"elapsed":7,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"c1462385-d837-4906-fbd5-77e520a4a53d"},"source":["# ... and characters\n","pipeline_char = Pipeline([\n","    ('vect', CountVectorizer(max_features=30000, analyzer='char', stop_words=None)),\n","    ('clf', MultinomialNB()),\n","])\n","pipeline_char.fit(train_texts_splt, train_labels_splt)\n","print('Score:', pipeline_char.score(val_texts, val_labels))"],"id":"Bwbzp_pg9EDy","execution_count":182,"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.62\n"]}]},{"cell_type":"markdown","metadata":{"id":"tVYKT73Z9EDz"},"source":["#### Tf-idf:\n","\n","This is the product of the frequency of the term (TF) and its inverse frequency in documents (IDF).\n","This method is usually used to measure the importance of a term $i$ in a document $j$ relative to the rest of the corpus, from a matrix of occurrences $ words \\times documents$. Thus, for a matrix $\\mathbf{T}$ of $|V|$ terms and $D$ documents:\n","$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n","\n","$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n","\n","$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n","\n","It can be adapted to our case by considering that the context of the second word is the document. However, TF-IDF is generally better suited to low-density matrices, since it will penalize terms that appear in a large part of the documents. "],"id":"tVYKT73Z9EDz"},{"cell_type":"code","metadata":{"id":"DAleskAW9EDz","executionInfo":{"status":"ok","timestamp":1635970364823,"user_tz":-60,"elapsed":188,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from sklearn.feature_extraction.text import TfidfTransformer"],"id":"DAleskAW9EDz","execution_count":183,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0OIXy4H9ED0","colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"status":"ok","timestamp":1635970365511,"user_tz":-60,"elapsed":521,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"348a6629-d016-4a41-fdc9-8d9aba24cc49"},"source":["# Fit and test a pipeline with tf-idf\n","\n","pipe = Pipeline([('count', CountVectorizer()),\n","                  ('tfid', TfidfTransformer())])\n","pipe.fit(train_texts_splt, train_labels_splt)\n","val_texts_transformed = pipe.transform(val_texts)\n","y_pred = multinomial_nb.predict(val_texts_transformed)\n","\n","cm = confusion_matrix(val_labels, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=multinomial_nb.classes_)\n","disp.plot()\n","plt.show()\n","print(classification_report(val_labels, y_pred))\n","print('Score:', multinomial_nb.score(val_texts_transformed, val_labels))"],"id":"X0OIXy4H9ED0","execution_count":184,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.81      0.78      0.79        27\n","           1       0.75      0.78      0.77        23\n","\n","    accuracy                           0.78        50\n","   macro avg       0.78      0.78      0.78        50\n","weighted avg       0.78      0.78      0.78        50\n","\n","Score: 0.78\n"]}]},{"cell_type":"markdown","metadata":{"id":"01EZrfCD9ED0"},"source":["### Pre-processing tools: NLTK\n","\n","We are now going to pre-process our textual data in order to simplify the model's task. **Note that this still will only be useful if we do not have a lot of training data to begin with !**\n","\n","**Important:** Until now, we obtained representations with ```vectorizer``` applied on the full document. The class divided the string into words itself ! Now, we'll need to do the **tokenization** ourselves in order to apply pre-processing to words. The simplest way to do this is to divide the string following blank spaces, with:\n","```text.split()```. If we need something more elaborate, we can use a tokenizer from NLTK, ```word_tokenize```."],"id":"01EZrfCD9ED0"},{"cell_type":"markdown","metadata":{"id":"JS7-FejW9ED0"},"source":["### Stemming \n","\n","Allows to go back to the root of a word: you can group different words around the same root, which facilitates generalization. Use:\n","```from nltk import SnowballStemmer```"],"id":"JS7-FejW9ED0"},{"cell_type":"code","metadata":{"id":"S4wE4RnL9ED1","executionInfo":{"status":"ok","timestamp":1635970371614,"user_tz":-60,"elapsed":228,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from nltk import SnowballStemmer\n","stemmer = SnowballStemmer(\"english\")"],"id":"S4wE4RnL9ED1","execution_count":185,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxBE7Ji19ED1"},"source":["**Example:**"],"id":"SxBE7Ji19ED1"},{"cell_type":"code","metadata":{"id":"oD20aRRC9ED1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970373000,"user_tz":-60,"elapsed":206,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"1bd654f4-72c7-44e2-ebef-68a730291703"},"source":["words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n","for word in words:\n","    print('word : %s ; stemmed : %s' %(word, stemmer.stem(word)))#.decode('utf-8'))))"],"id":"oD20aRRC9ED1","execution_count":186,"outputs":[{"output_type":"stream","name":"stdout","text":["word : singers ; stemmed : singer\n","word : cat ; stemmed : cat\n","word : generalization ; stemmed : general\n","word : philosophy ; stemmed : philosophi\n","word : psychology ; stemmed : psycholog\n","word : philosopher ; stemmed : philosoph\n"]}]},{"cell_type":"markdown","metadata":{"id":"j_Y3jhac9ED2"},"source":["**Data transformation:**"],"id":"j_Y3jhac9ED2"},{"cell_type":"code","metadata":{"id":"9jidXa4h9ED2","executionInfo":{"status":"ok","timestamp":1635970374497,"user_tz":-60,"elapsed":169,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["def stem(X):\n","    #\n","    # To complete \n","    #\n","    X_stem = []\n","    for doc in X:\n","        words = doc.split()\n","        new_doc = ''\n","        for word in words:\n","            word_stem = stemmer.stem(word)\n","            new_doc += (word_stem+' ')\n","        X_stem.append(new_doc[:-1]) # avoids last blank space character\n","    return X_stem"],"id":"9jidXa4h9ED2","execution_count":187,"outputs":[]},{"cell_type":"code","metadata":{"id":"-RylJ5-n9ED2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970376316,"user_tz":-60,"elapsed":835,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"52dd8d73-85af-4ee7-fb3a-a60d67d54740"},"source":["# Transform training and validation data and test it with a basic pipeline\n","train_stem = stem(train_texts_splt)\n","val_stem = stem(val_texts)\n","\n","pipeline_base = Pipeline([\n","    ('vect', CountVectorizer(max_features=30000, analyzer='word', stop_words=None)),\n","    ('clf', MultinomialNB()),\n","])\n","# Fit and test the pipeline\n","pipeline_base.fit(train_stem, train_labels_splt)\n","print('Score:', pipeline_base.score(val_stem, val_labels))"],"id":"-RylJ5-n9ED2","execution_count":188,"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.84\n"]}]},{"cell_type":"markdown","metadata":{"id":"qG8OX7Ny9ED2"},"source":["### Part of speech tags\n","\n","To generalize, we can also use the Part of Speech (POS) of the words, which will allow us to filter out information that is potentially not useful to the model. We will retrieve the POS of the words using the functions:\n","```from nltk import pos_tag, word_tokenize```"],"id":"qG8OX7Ny9ED2"},{"cell_type":"code","metadata":{"id":"gQWK55959ED3","executionInfo":{"status":"ok","timestamp":1635970377599,"user_tz":-60,"elapsed":177,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["import nltk\n","from nltk import pos_tag, word_tokenize"],"id":"gQWK55959ED3","execution_count":189,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zTPPRsNw9ED3"},"source":["**Example:**"],"id":"zTPPRsNw9ED3"},{"cell_type":"code","metadata":{"id":"vFu36AgO9ED3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970379304,"user_tz":-60,"elapsed":528,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"6c54a00b-f10f-4708-910e-bfe20bd23541"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","pos_tag(word_tokenize(('I am Sam')))"],"id":"vFu36AgO9ED3","execution_count":190,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["[('I', 'PRP'), ('am', 'VBP'), ('Sam', 'NNP')]"]},"metadata":{},"execution_count":190}]},{"cell_type":"markdown","metadata":{"id":"K3siZ7WO9ED3"},"source":["**Data transformation:** only keep nouns, verbs, adverbs, and adjectives (```['NN', 'VB', 'ADJ', 'RB']```) for our model."],"id":"K3siZ7WO9ED3"},{"cell_type":"code","metadata":{"id":"s8XgyXOt9ED3","executionInfo":{"status":"ok","timestamp":1635970380655,"user_tz":-60,"elapsed":169,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["def pos_tag_filter(X, good_tags=['NN', 'VB', 'JJ', 'RB']):\n","    #\n","    # To complete \n","    #\n","    X_postag = []\n","    for doc in X:\n","        words_pos_tags = pos_tag(word_tokenize(doc))\n","        new_doc = ''\n","        for pair in words_pos_tags:\n","            if pair[1] in good_tags:\n","                new_doc += (pair[0]+' ')\n","        X_postag.append(new_doc[:-1]) # avoids last blank space character\n","\n","    return X_postag"],"id":"s8XgyXOt9ED3","execution_count":191,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rZSsuAN9ED4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970385254,"user_tz":-60,"elapsed":3627,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"8ffc945a-8ae5-4424-ef37-703314178a48"},"source":["# Transform training and validation data and test it with a basic pipeline\n","train_postag = pos_tag_filter(train_texts_splt)\n","val_postag = pos_tag_filter(val_texts)\n","\n","pipeline_base = Pipeline([\n","    ('vect', CountVectorizer(max_features=30000, analyzer='word', stop_words=None)),\n","    ('clf', MultinomialNB()),\n","])\n","# Fit and test the pipeline\n","pipeline_base.fit(train_postag, train_labels_splt)\n","print('Score:', pipeline_base.score(val_postag, val_labels))"],"id":"3rZSsuAN9ED4","execution_count":192,"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.82\n"]}]},{"cell_type":"markdown","metadata":{"id":"UDwqOMO_9ED4"},"source":["# Dense Representations \n","\n","##  Word Embeddings : Distributed representations via the distributional hypothesis \n","\n","**Goal**: We will try to obtain dense representations (as vectors of real numbers) of words (and possibly sentences). These representations are intended to be distributed: they are non-local representations. We represent an object as a combination of *features*, as opposed to the attribution of a dedicated symbol: see the founding work of Geoffrey Hinton, among others, on the subject: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n","\n","The term *distributed* representations is very general, but is what we are looking for. The challenge is therefore to be able to build, automatically, such representations.\n","\n","**Underlying idea**: It is based on the distributional hypothesis: contextual information is sufficient to obtain a viable representation of linguistic objects.\n"," - For a large class of cases [...] the meaning of a word is its use in the language.\" Wittgenstein (Philosophical Investigations, 43 - 1953)\n"," - You shall know a word by the company it keeps, Firth.\n","\n","Thus, a word can be characterized by the words that accompany it, via co-occurrence counts. Two words with a similar meaning will have a similar contextual distribution and are therefore more likely to appear in similar contexts. This hypothesis can be used as a justification for the application of statistics to semantics (information extraction, semantic analysis). It also allows some form of generalization: we can assume that the information we have about a word will be generalized to words with a similar distribution. \n","\n","**Motivation**: The goal is to obtain distributed representations in order to be able to effectively**:\n","- Directly perform a semantic surface analysis.\n","- Use it as a source of information for other language-related models and applications, especially for sentiment analysis. \n","\n","\n","**Terminology**: Be careful not to confuse the idea of *distributed* and *distributional* representation. The latter generally indicates (for words) that the representation has been obtained strictly from co-occurrence counts, whereas additional information (document labels, part of speech tags, ...) can be used to build distributed representations. \n","The models that allow to build these dense representations, in the form of vectors, are often called *vector spaces models*. These representations are also regularly called *word embeddings*, because the words are embedded in a vector space. In French, we often find the term *word embedding* or *lexical embedding*."],"id":"UDwqOMO_9ED4"},{"cell_type":"markdown","metadata":{"id":"DhhQPzmi9ED4"},"source":["## Getting representations: counts of occurrences and co-occurrences\n","\n","Depending on the type of corpus available, different types of distributional information can be obtained. If we have access to a collection of documents, we can thus choose to count the number of occurrences of each word in each document, to obtain a $words \\times documents$ matrix: it is on this principle that **Tf-Idf** is built. We will now look at a more general case: we have a large amount of data in text form, and we want to obtain representations of words in the form of vectors of reduced size, without the need to divide them into documents or categories. \n","\n","Suppose we have a corpus containing $T$ different words. We will construct a $\\mathbf{M}$ matrix of size $T \\times T$ which will contain the number of co-occurrences between words. There will be different factors to consider when constructing this matrix: \n","\n","- How do you define the 'context' of a word - context which will tell you what terms co-occur with that word?\n","\n","We can choose to use different scales: the document, the sentence, the nominal group, or simply a window of $k$ words, depending on the information we want to capture.\n","\n","\n","- How do we quantify the importance of the counts? \n","\n","$\\rightarrow$ For example, we can give a decreasing weight to a co-occurrence according to the distance between the two words concerned ($\\frac{1}{d+1}$ for a separation by $d$ words).\n","\n","\n","- Should we keep all the words that appear in the corpus? \n","\n","$\\rightarrow$ Usually not. We will see that for large corpora, the number $T$ of different words is huge. Second, even if the number of words is reasonable, we will have very little distributional information on the rarest words, and the representation obtained will be of poor quality. We will have to ask ourselves how to filter these words, and how to treat the words we choose not to represent.  "],"id":"DhhQPzmi9ED4"},{"cell_type":"markdown","metadata":{"id":"95BWIa4k9ED4"},"source":["#### Example:\n","\n","Let's look at the following text:\n","\n","*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n","\n","We choose to define the context of a word as the sentence to which it belongs, and to not use any weighting.\n","We obtain the following matrix: \n","\n","|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n","|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n","| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n","| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n","| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n","| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n","| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n","| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n","| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n","| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n","| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"],"id":"95BWIa4k9ED4"},{"cell_type":"markdown","metadata":{"id":"s5LOdOsd9ED5"},"source":["## Modifying the representations:\n","\n","We may want to alter the representations to obtain better features - depending on what use we will have for them.\n","\n","**Normalization**: Very easy: we want to cancel the influence of the magnitude of the counts on the representation.\n","\n","$$\\mathbf{m_{normalized}} = \\left[ \n","   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n","   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n","   \\ldots\n","   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n","\\right]$$\n"," \n","**Pointwise Mutual Information**: The aim is to assess the extent to which the co-occurrence of the two terms is *unexpected*. This measure is the ratio of the joint probability of the two words and the product of their individual probabilities:\n","$$\n","\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n","$$\n","The joint probability of the two words corresponds to the number of times they are observed together, divided by the total number of co-occurrences in the corpus: \n","$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n","The individual probability of a word simply corresponds to its frequency, which can be calculated by counting all co-occurrences where that word appears:\n","$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n","Hence,\n","$$ \n","\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n","$$\n","We thus calculate the discrepancy between the observation we have made in our corpus and the frequency of appearance of these terms if we consider them independent - i.e. we assume that their co-occurrence is a coincidence.\n","\n","The main problem with this measure is that it is not adapted to the case where no co-occurrence is observed. Since the PMI is supposed to return a positive quantity if more co-occurrences are observed than expected, and a negative quantity if fewer co-occurrences are observed, we cannot choose to replace $\\log(0)$ by $0$. A commonly used solution is to use the **Positive PMI**, which sets all negative values to $0$.\n"," \n"," $$\\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n"," \\begin{cases}\n"," \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n"," 0 & \\textrm{otherwise}\n"," \\end{cases}$$"],"id":"s5LOdOsd9ED5"},{"cell_type":"markdown","metadata":{"id":"y1ZpKrzh9ED5"},"source":["### Co-occurences matrix : reducing the dimension\n","\n","#### Motivation\n","\n","The aim is not only to reduce the size of the data (thus, we will deal with vectors of reduced dimensions, rather than working with vectors of the size of the vocabulary) but also to highlight higher level relationships between words: by reducing their representations to the *most important* dimensions of the data, we *generalize* certain properties between words.\n","\n","#### Dimension reduction via SVD \n","\n","A matrix is a linear transformation: applying an SVD to it means decomposing our linear transformation into a product of linear transformations of different types. In fact, we will change the basis of our vector, and replace our data in a space where each of the coordinates are unchanged by the transformation carried out. Thus, we decompose the matrix $\\mathbf{M}$ into three matrices:\n","\n","$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n","\n","Matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, et $\\mathbf{V}$ have the following properties:\n","- $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ and $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). They contain the eigen vectors to the right and to the left of $\\mathbf{M}$.\n","- $\\mathbf{\\lambda}$ is a diagonal matrix: careful, it's not necessarily square. Values on the diagonal are the eigenvalues of $\\mathbf{M}$.\n","\n","Thus, the *most important* dimensions correspond to the largest eigenvalues. Reducing our data to $k$ dimensions corresponds to keeping only the vectors corresponding to the first $k$ eigenvalues - and this is equivalent to taking the first $k$ vectors of the $U$ matrix. \n","\n","Note: When we apply this method to the matrix of $\\mathbf{M}$ counts of dimension $T \\times D$, where $\\mathbf{M}_{t,d}$ contains the number of occurrences of the word $t$ in the document $d$, we obtain the method called **Latent Semantic Analysis**, for the detection of latent (semantic) components allowing the grouping of documents.  "],"id":"y1ZpKrzh9ED5"},{"cell_type":"markdown","metadata":{"id":"VhdsoldR9ED5"},"source":["### In practice: get a Vocabulary.\n","\n","To begin, we will implement separately a function returning the vocabulary. Here we will have to be able to control its size, either by indicating a maximum number of words, or a minimum number of occurrences to take the words into account. We add, at the end, an \"unknown\" word that will replace all the words that do not appear in our \"limited\" vocabulary. \n","\n","Reminder: use tokenization to obtain words from a document !"],"id":"VhdsoldR9ED5"},{"cell_type":"code","metadata":{"id":"JuPbEawg9ED6","executionInfo":{"status":"ok","timestamp":1635970392253,"user_tz":-60,"elapsed":187,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["def vocabulary(corpus, count_threshold=1, voc_threshold=0):\n","    \"\"\"    \n","    Function using word counts to build a vocabulary - can be improved with a second parameter for \n","    setting a frequency threshold\n","    Params:\n","        corpus (list of list of strings): corpus of sentences\n","        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n","        voc_threshold (int): maximum size of the vocabulary \n","    Returns:\n","        vocabulary (dictionary): keys: list of distinct words across the corpus\n","                                 values: indexes corresponding to each word sorted by frequency        \n","    \"\"\"\n","    #\n","    # To complete \n","    #   \n","    word_counts = {}\n","    for doc in corpus:\n","        words = doc.split()\n","        for word in words:\n","            if word not in word_counts:\n","                word_counts[word] = 1\n","            else:\n","                word_counts[word] += 1\n","    word_counts = dict(sorted(word_counts.items(), key=lambda x: x[1], reverse=True))\n","    \n","    filtered_word_counts = {}\n","    for key in word_counts:\n","        if word_counts[key] >= count_threshold:\n","            filtered_word_counts[key] = word_counts[key]\n","        if voc_threshold != 0:\n","            if len(filtered_word_counts) == voc_threshold:\n","                break\n","    \n","    filtered_word_counts['UNK'] = 0\n","    vocabulary = {}\n","    vocabulary_word_counts = {}\n","    vocabulary_word_counts = filtered_word_counts\n","    \n","    for index, (key, value) in enumerate(filtered_word_counts.items()):\n","        vocabulary[key] = index\n","    vocabulary['UNK'] = len(vocabulary)-1\n","    \n","    return vocabulary, vocabulary_word_counts"],"id":"JuPbEawg9ED6","execution_count":193,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_Bu0kzv9ED6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635970393680,"user_tz":-60,"elapsed":240,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"c353e724-2f7d-4018-cb7a-2d28bc927a96"},"source":["# Example for testing:\n","\n","corpus = ['I walked down down the boulevard',\n","          'I walked down the avenue',\n","          'I ran down the boulevard',\n","          'I walk down the city',\n","          'I walk down the the avenue']\n","\n","voc, counts = vocabulary(corpus, count_threshold = 3)\n","print(voc)\n","print(counts)\n","\n","# We expect something like this:\n","#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n","#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n","\n","voc, counts = vocabulary(corpus)\n","print(voc)\n","print(counts)\n","\n","# We expect something like this:\n","#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n","#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"],"id":"n_Bu0kzv9ED6","execution_count":194,"outputs":[{"output_type":"stream","name":"stdout","text":["{'down': 0, 'the': 1, 'I': 2, 'UNK': 3}\n","{'down': 6, 'the': 6, 'I': 5, 'UNK': 0}\n","{'down': 0, 'the': 1, 'I': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n","{'down': 6, 'the': 6, 'I': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}\n"]}]},{"cell_type":"markdown","metadata":{"id":"86YvJQ-G9ED6"},"source":["#### Application to a real data set\n","\n","We're going to work with the **imdb** data.\n","\n","#### Quick study of the data\n","\n","We would like to get an idea of what's in these film reviews before we proceed. So we'll get the vocabulary (in full) and represent the frequencies of the words, in order (be careful, you'll have to use a logarithmic scale): we should find back Zipf's law. This will give us an idea of the size of the vocabulary we will be able to choose: it's a matter of making a compromise between the necessary resources (size of the objects in memory) and the amount of information we can get from them (rare words can bring a lot of information, but it's difficult to learn good representations of them, because they are rare!).  "],"id":"86YvJQ-G9ED6"},{"cell_type":"code","metadata":{"id":"g9bFhtVB9ED7","colab":{"base_uri":"https://localhost:8080/","height":706},"executionInfo":{"status":"ok","timestamp":1635975218613,"user_tz":-60,"elapsed":1500,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"9b612670-6c90-4b2a-b303-5846345a20b5"},"source":["# We would like to display the curve of word frequencies given their rank (index) in the vocabulary\n","# texts = corpus\n","texts = train_texts_splt\n","vocab, word_counts = vocabulary(texts)\n","#\n","#  To fill in !\n","#\n","# We can for example use the function plt.scatter()\n","plt.figure(figsize=(20,5))\n","plt.title('Word counts versus rank')\n","plt.scatter(vocab.values(), word_counts.values())\n","plt.yscale('log')\n","plt.ylim(1,10000)\n","plt.show()\n","# We would like to know how much of the data is represented by the 'k' most frequent words\n","print('Vocabulary size: %i' % len(vocab))\n","print('Part of the corpus by taking the \"x\" most frequent words ?')\n","\n","texts = train_texts_splt\n","vocab, word_counts = vocabulary(texts, voc_threshold = 5000)\n","plt.figure(figsize=(20,5))\n","plt.title('Word counts versus rank')\n","plt.scatter(vocab.values(), word_counts.values())\n","plt.yscale('log')\n","plt.ylim(1,10000)\n","plt.show()\n","print('Vocabulary size: %i' % len(vocab))"],"id":"g9bFhtVB9ED7","execution_count":318,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x360 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Vocabulary size: 10242\n","Part of the corpus by taking the \"x\" most frequent words ?\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x360 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Vocabulary size: 5001\n"]}]},{"cell_type":"markdown","metadata":{"id":"_km4eqhS9ED7"},"source":["**Result of the analysis**: we can be satisfied with a vocabulary of 10,000 or even 5,000 words - this is important, because it will determine the size of the objects we will manipulate. "],"id":"_km4eqhS9ED7"},{"cell_type":"code","metadata":{"id":"ImbiTC5J9ED7","executionInfo":{"status":"ok","timestamp":1635972383006,"user_tz":-60,"elapsed":2544,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["vocab_5k, word_counts_5k = vocabulary(train_texts, 0, 5000)"],"id":"ImbiTC5J9ED7","execution_count":269,"outputs":[]},{"cell_type":"code","metadata":{"id":"2O6Pqg2l9ED7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972383006,"user_tz":-60,"elapsed":10,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"8a120681-784c-473a-b74c-0f58da79ef01"},"source":["print(vocab_5k['cinema'])"],"id":"2O6Pqg2l9ED7","execution_count":270,"outputs":[{"output_type":"stream","name":"stdout","text":["719\n"]}]},{"cell_type":"markdown","metadata":{"id":"Lk79ZjYt9ED8"},"source":["We could here compute the co-occurence matrix, and then reduce its dimension. Instead, we will use two of the most popular methods used to produce dense word representations (word embeddings). These methods are very different in practice, but are conceptually close, and resemble the procedure described earlier: reducing the dimension of co-occurences metrics.\n","\n","## Getting a representation: commonly used algorithms\n","\n","The idea here is to define a set of representations ${w_{i}}_{i=1}^{V}$, of predefined dimension $d$ (here, we will work with $d = 300$), for all the words $i$ of the vocabulary $V$ - then **train** these representations to match what we want. "],"id":"Lk79ZjYt9ED8"},{"cell_type":"markdown","metadata":{"id":"lcATAnic9ED8"},"source":["### Word2Vec\n","\n","\n","####  The skip-gram model\n","\n","The basic skip-gram model estimates the probabilities of a pair of words $(i, j)$ to appear together in data:\n","\n","$$P(j \\mid i) = \\frac{\\exp(w_{i} c_{j})}{\\sum_{j'\\in V}\\exp(w_{i} c_{j'})}$$\n","\n","\n","where $w_{i}$ is the lign vector (of the word) $i$ and $c_{j}$ is the column vector (of a context word) $j$. The objective is to minimize the following quantity:\n","\n","\n","$$ -\\sum_{i=1}^{m} \\sum_{k=1}^{|V|} \\textbf{1}\\{o_{i}=k\\} \\log \\frac{\\exp(w_{i} c_{k})}{\\sum_{j=1}^{|V|} \\exp(w_{i} c_{j})}$$\n","\n","\n","where $V$ is the vocabulary.\n","The inputs $w_{i}$ are the representations of the words, which are updated during training, and the output is an *one-hot* $o$ vector, which contains only one $1$ and $0$. For example, if `good` is the 47th word in the vocabulary, the output $o$ for an example or `good` is the word to predict will consist of $0$s everywhere except $1$ in the 47th position of the vector. `good` will be the word to predict when the input $w$ is a word in its context.\n","We therefore obtain this output with standard softmax - we add a bias term $b$ .\n","\n","\n","$$ o = \\textbf{softmax}(w_{a}C + b)$$\n","\n","\n","If we use the set of representations for the whole vocabulary (the matrix $W$) as input, we get:\n","\n","\n","$$ O = \\textbf{softmax}(WC + b)$$\n","\n","\n","and so we come back to the central idea of all our methods: we seek to obtain word representations from co-occurrence counts. Here, we train the parameters contained in $W$ and $C$, two matrices representing the words in reduced dimension (300) so that their scalar product is as close as possible to the co-occurrences observed in the data, using a maximum likelihood objective.\n","\n","#### Skip-gram with negative sampling\n","\n","The training of the skip-gram model implies to calculate a sum on the whole vocabulary, because of the **softmax**. As soon as the size of the vocabulary increases, it becomes impossible to compute. In order to make the calculations faster, we change the objective and use the method of *negative sampling* (or, very close to it, the *noise contrastive estimation*).\n","\n","\n","If we note $\\mathcal{D}$ the data set and we note $\\mathcal{D}'$ a set of pairs of words that are **not** in the data (and that in practice, we draw randomly), the objective is:\n","\n","\n","$$\\sum_{i, j \\in \\mathcal{D}}-\\log\\sigma(w_{i}c_{j}) + \\sum_{i, j \\in \\mathcal{D}'}\\log\\sigma(w_{i}c_{j})$$\n","\n","\n","where $\\sigma$ is the sigmoid activation function $\\frac{1}{1 + \\exp(-x)}$.\n","A common practice is to generate pairs from $\\mathcal{D}'$ in proportion to the frequencies of the words in the training data (the so-called unigram distribution):\n","\n","\n","$$P(w) = \\frac{\\textbf{T}(w)^{0.75}}{\\sum_{w'\\in V} \\textbf{T}(w')}$$\n","\n","\n","Although different, this new objective function is a sufficient approximation of the previous one, and is based on the same principle. Much research has been done on this objective: for example, [Levy and Golberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) shows that the objective calculates the PMI matrix shifted by a constant value. One can also see [Cotterell et al. 2017](https://aclanthology.coli.uni-saarland.de/papers/E17-2028/e17-2028) for an interpretation of the algorithm as a variant of PCA."],"id":"lcATAnic9ED8"},{"cell_type":"markdown","metadata":{"id":"BAA7j8Ho9ED8"},"source":["We will use the ```gensim``` library for its implementation of word2vec in python. We'll have to make a specific use of it, since we want to keep the same vocabulary as before: we'll first create the class, then get the vocabulary we generated above. \n","To avoid having to put all the data in memory all at once, we define a generator, which will take all the input data and pre-process it, and return to the ```Word2Vec``` class sentence by sentence. "],"id":"BAA7j8Ho9ED8"},{"cell_type":"code","metadata":{"id":"8Vd8Pb6q9ED8","executionInfo":{"status":"ok","timestamp":1635972383985,"user_tz":-60,"elapsed":261,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from gensim.models import Word2Vec\n","\n","model = Word2Vec(vector_size=300,\n","                 window=5,\n","                 null_word=len(word_counts_5k))\n","model.build_vocab_from_freq(word_counts_5k)"],"id":"8Vd8Pb6q9ED8","execution_count":271,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dr7VyDb49ED9","executionInfo":{"status":"ok","timestamp":1635972428045,"user_tz":-60,"elapsed":44062,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["preprocessed_corpus = [word_tokenize(train_texts[i]) for i in range(len(train_texts))]"],"id":"Dr7VyDb49ED9","execution_count":272,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQ1oEUMn9ED9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972649719,"user_tz":-60,"elapsed":221709,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"e5ae23bf-0bbf-4267-c780-81ea37a61bb1"},"source":["model.train(preprocessed_corpus, total_examples=len(train_texts), epochs=20, report_delay=1)"],"id":"uQ1oEUMn9ED9","execution_count":273,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(86774799, 141130640)"]},"metadata":{},"execution_count":273}]},{"cell_type":"code","metadata":{"id":"sXJ_e2zN9ED9","executionInfo":{"status":"ok","timestamp":1635972649720,"user_tz":-60,"elapsed":15,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["W2VEmbeddings = model.wv.vectors"],"id":"sXJ_e2zN9ED9","execution_count":274,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9cPb-ebf9ED9"},"source":["### Glove\n","\n","The objective defined by Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) is to learn from the vectors $w_{i}$ and $w_{k}$ so that their scalar product corresponds to the logarithm of their **Pointwise Mutual Information**: \n","\n","\n","$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n","\n","\n","In the article, this objective is carefully justified by a reasoning about the operations one wants to perform with these vectors and the properties they should have - in particular, symmetry between rows and columns (see the article for more details).  \n","The final goal obtained is the following, where $M$ is the co-occurrence matrix:\n","\n","\n","$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n","  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n","  \n"," \n","Here, $f$ is a *scaling* function that reduces the importance of the most frequent co-occurrence counts: \n","\n","\n","$$f(x) \n","\\begin{cases}\n","(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n","1 & \\textrm{otherwise}\n","\\end{cases}$$\n","\n","\n","Usually, we choose $\\alpha=0.75$ and $x_{\\max} = 100$, although these parameters may need to be changed depending on the data."],"id":"9cPb-ebf9ED9"},{"cell_type":"markdown","metadata":{"id":"z1xopB0S9ED9"},"source":["The following code uses the gensim API to retrieve pre-trained representations (It is normal that the loading is long)."],"id":"z1xopB0S9ED9"},{"cell_type":"code","metadata":{"id":"VmejesaE9ED-","executionInfo":{"status":"ok","timestamp":1635972788009,"user_tz":-60,"elapsed":138302,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["import gensim.downloader as api\n","loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"],"id":"VmejesaE9ED-","execution_count":275,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xh4JXbDS9ED-"},"source":["We can extract the embedding matrix this way, and check its size:"],"id":"xh4JXbDS9ED-"},{"cell_type":"code","metadata":{"id":"bVwCa_q29ED-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972788010,"user_tz":-60,"elapsed":32,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"19b138b7-bd9c-4ecc-fc73-f010f2f989fc"},"source":["loaded_glove_embeddings = loaded_glove_model.vectors\n","print(loaded_glove_embeddings.shape)"],"id":"bVwCa_q29ED-","execution_count":276,"outputs":[{"output_type":"stream","name":"stdout","text":["(400000, 300)\n"]}]},{"cell_type":"markdown","metadata":{"id":"-zzqizxK9ED-"},"source":["We can see that there are $400,000$ words represented, and that the embeddings are of size $300$. We define a function that returns, from the loaded model, the vocabulary and the embedding matrix according to the structures we used before. We add, here again, an unknown word \"UNK\" in case there are words in our data that are not part of the $400,000$ words represented here. "],"id":"-zzqizxK9ED-"},{"cell_type":"code","metadata":{"id":"OwUBbRKL9ED_","executionInfo":{"status":"ok","timestamp":1635972788010,"user_tz":-60,"elapsed":22,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["def get_glove_voc_and_embeddings(glove_model):\n","    voc = {word : index for word, index in enumerate(glove_model.index_to_key)}\n","    voc['UNK'] = len(voc)\n","    embeddings = glove_model.vectors\n","    return voc, embeddings"],"id":"OwUBbRKL9ED_","execution_count":277,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGOAqA7a9ED_","executionInfo":{"status":"ok","timestamp":1635972788012,"user_tz":-60,"elapsed":24,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"],"id":"BGOAqA7a9ED_","execution_count":278,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqfXDftc9ED_"},"source":["In order to compare the representations loaded here and the ones produced with word2vec, the same vocabulary should be used. For this purpose, I reuse the following code to create a $5000$ word vocabulary from the data, and I add at the end a function that returns the matrix of representations loaded with Glove for these $5000$ words only, in the right order. "],"id":"oqfXDftc9ED_"},{"cell_type":"code","metadata":{"id":"0SKbRp2-9ED_","executionInfo":{"status":"ok","timestamp":1635972788012,"user_tz":-60,"elapsed":23,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["def get_glove_adapted_embeddings(glove_model, input_voc):\n","    keys = {i: glove_model.key_to_index.get(w, None) for w, i in input_voc.items()}\n","    index_dict = {i: key for i, key in keys.items() if key is not None}\n","    embeddings = np.zeros((len(input_voc),glove_model.vectors.shape[1]))\n","    for i, ind in index_dict.items():\n","        embeddings[i] = glove_model.vectors[ind]\n","    return embeddings"],"id":"0SKbRp2-9ED_","execution_count":279,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fn50Ofa79ED_","executionInfo":{"status":"ok","timestamp":1635972788303,"user_tz":-60,"elapsed":313,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"],"id":"Fn50Ofa79ED_","execution_count":280,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"emRjW32N9EEA"},"source":["This function takes as input the model loaded using the Gensim API, as well as a vocabulary we created ourselves, and returns the embedding matrix from the loaded model, for the words in our vocabulary and in the right order.\n","Note: unknown words are represented by a vector of zeros:"],"id":"emRjW32N9EEA"},{"cell_type":"code","metadata":{"id":"YA0EGvXe9EEA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972788304,"user_tz":-60,"elapsed":30,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"9b47a601-97db-4a8f-ef19-f82606496284"},"source":["print(GloveEmbeddings.shape)\n","GloveEmbeddings[vocab_5k['UNK']]"],"id":"YA0EGvXe9EEA","execution_count":281,"outputs":[{"output_type":"stream","name":"stdout","text":["(5001, 300)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{},"execution_count":281}]},{"cell_type":"markdown","metadata":{"id":"cBloHnsc9EEA"},"source":["### Comparing vectors\n","\n","These very large vectors can be used for a very basic semantic analysis: for example, by searching for the closest neighbors of a word. However, one must be careful with the distances used, related to certain metrics (Euclidean, Cosine) or possibly others related to belonging to sets (Matching, Jaccard). The normalization of vectors can also play a role. In any case, care must be taken not to over-interpret such results. "],"id":"cBloHnsc9EEA"},{"cell_type":"code","metadata":{"id":"gD8ZbfdT9EEA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972788498,"user_tz":-60,"elapsed":212,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"b76ed58d-eced-4b86-df93-d61c9f952abc"},"source":["def euclidean(u, v):\n","    return np.linalg.norm(u-v)\n","\n","def length_norm(u):\n","    return u / np.sqrt(u.dot(u))\n","\n","def cosine(u, v):\n","    return 1.0 - length_norm(u).dot(length_norm(v))\n","\n","from sklearn.neighbors import NearestNeighbors\n","\n","def print_neighbors(distance, voc, co_oc, mot, k=10):\n","    inv_voc = {id: w for w, id in voc.items()}\n","    neigh = NearestNeighbors(k, algorithm='brute', metric=distance)\n","    neigh.fit(co_oc) \n","    dist, ind = neigh.kneighbors([co_oc[voc[mot]]])\n","    print(\"Plus proches voisins de %s selon la distance '%s': \" % (mot, distance.__name__))\n","    print([[inv_voc[i] for i in s[1:]] for s in ind])\n","    \n","print_neighbors(euclidean, vocab_5k, W2VEmbeddings, 'good')\n","print_neighbors(cosine, vocab_5k, W2VEmbeddings, 'good')\n","\n","print_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good')\n","print_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good')"],"id":"gD8ZbfdT9EEA","execution_count":282,"outputs":[{"output_type":"stream","name":"stdout","text":["Plus proches voisins de good selon la distance 'euclidean': \n","[['good.', 'expect', 'instead.', 'great', 'ridiculously', 'solid', 'cool', 'taste', 'bad']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['expect', 'good.', 'great', 'bad', 'instead.', 'cool', 'solid', 'nice', 'fine']]\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['better', 'well', 'always', 'really', 'sure', 'way', 'so', 'but', 'excellent']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['better', 'really', 'always', 'you', 'well', 'excellent', 'very', 'things', 'think']]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n","  \"\"\"\n"]}]},{"cell_type":"markdown","metadata":{"id":"Py5A9xRQ9EEB"},"source":["### Visualisation in two dimensions\n","\n","We will now use **principal component analysis** (PCA) to visualize our data in 2 dimensions.  This is equivalent to applying SVD to the covariance matrix of the data, so that the principal directions are independent of each other and maximize the variance of the data.\n","We use the class ```PCA``` from the ```scikit-learn``` package: "],"id":"Py5A9xRQ9EEB"},{"cell_type":"code","metadata":{"id":"J2eg18E29EEB","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1635972789045,"user_tz":-60,"elapsed":551,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"866a8610-22ab-4602-acf0-92fba8c25163"},"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2, whiten=True)\n","Emb = pca.fit_transform(GloveEmbeddings)\n","\n","words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n","         'dialog', 'role', 'actor', 'camera', 'scene',\n","         'film', 'movie', 'award']\n","ind_words = [vocab_5k[w] for w in words]\n","x_words = [Emb[ind,0] for ind in ind_words]\n","y_words = [Emb[ind,1] for ind in ind_words]\n","\n","fig, ax = plt.subplots()\n","ax.scatter(x_words, y_words)\n","\n","for i, w in enumerate(words):\n","    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"],"id":"J2eg18E29EEB","execution_count":283,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"_u_wm_c89EEB","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1635972789937,"user_tz":-60,"elapsed":897,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"f601f492-1c8e-49c5-ae31-0cc57d8b9266"},"source":["pca = PCA(n_components=2, whiten=True)\n","Emb = pca.fit_transform(W2VEmbeddings)\n","\n","words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n","         'dialog', 'role', 'actor', 'camera', 'scene',\n","         'film', 'movie', 'award']\n","ind_words = [vocab_5k[w] for w in words]\n","x_words = [Emb[ind,0] for ind in ind_words]\n","y_words = [Emb[ind,1] for ind in ind_words]\n","\n","fig, ax = plt.subplots()\n","ax.scatter(x_words, y_words)\n","\n","for i, w in enumerate(words):\n","    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"],"id":"_u_wm_c89EEB","execution_count":284,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"GF-WHSpbzl99"},"source":["**Results of projections**: We may observe that the words related to one another or belonging to the same category/field are trying to group together. For example, in the case of Word2Vec embeddings, the words *camera, actor, scene* are close to each other. For Glove embeddings, we can see a similar scenario, for example, the words *great* and *best* are closely similar."],"id":"GF-WHSpbzl99"},{"cell_type":"markdown","metadata":{"id":"d5cAE3IY9EEB"},"source":["### Application to sentiment analysis\n","\n","We will now use these representations for sentiment analysis. \n","The basic model, as before, will be constructed in two steps:\n","- A function to obtain vector representations of criticism, from text, vocabulary, and vector representations of words. Such a function (to be completed below) will associate to each word of a review its embeddings, and create the representation for the whole sentence by summing these embeddings.\n","- A classifier will take these representations as input and make a prediction. To achieve this, we can first use logistic regression ```LogisticRegression``` from ```scikit-learn```  "],"id":"d5cAE3IY9EEB"},{"cell_type":"code","metadata":{"id":"7XwCDH5g9EEC","executionInfo":{"status":"ok","timestamp":1635972789940,"user_tz":-60,"elapsed":17,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n","    \"\"\"\n","    Represent the sentences as a combination of the vector of its words.\n","    Parameters\n","    ----------\n","    texts : a list of sentences   \n","    vocabulary : dict\n","        From words to indexes of vector.\n","    embeddings : Matrix containing word representations\n","    np_func : function (default: np.sum)\n","        A numpy matrix operation that can be applied columnwise, \n","        like `np.mean`, `np.sum`, or `np.prod`. \n","    Returns\n","    -------\n","    np.array, dimension `(len(texts), embeddings.shape[1])`            \n","    \"\"\"\n","    #\n","    # To complete\n","    #   \n","    representations = np.zeros([len(texts), embeddings.shape[1]])\n","    for i in range(representations.shape[0]):\n","        words = texts[i].split()\n","        for word in words:\n","            if word not in vocabulary:\n","                word = 'UNK'\n","            representations[i] = np_func([representations[i], embeddings[vocabulary[word]]], axis=0)\n","        \n","    return representations"],"id":"7XwCDH5g9EEC","execution_count":285,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0hJ50VC9EEC","executionInfo":{"status":"ok","timestamp":1635972790585,"user_tz":-60,"elapsed":661,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["train_rep = sentence_representations(train_texts_splt, vocab_5k, GloveEmbeddings)\n","val_rep = sentence_representations(val_texts, vocab_5k, GloveEmbeddings)"],"id":"f0hJ50VC9EEC","execution_count":286,"outputs":[]},{"cell_type":"code","metadata":{"id":"wfN9H-6e9EEC","executionInfo":{"status":"ok","timestamp":1635972790823,"user_tz":-60,"elapsed":245,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["from sklearn.linear_model import LogisticRegression"],"id":"wfN9H-6e9EEC","execution_count":287,"outputs":[]},{"cell_type":"code","metadata":{"id":"re9CeAC-9EEC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972790825,"user_tz":-60,"elapsed":24,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"09fa395c-7320-4141-a4f6-6985362ab744"},"source":["# Fit the classifier on the transformed training data and test it on the transformed validation data\n","clf = LogisticRegression().fit(train_rep, train_labels_splt)\n","print(clf.predict(val_rep))\n","print('Score:', clf.score(val_rep, val_labels))"],"id":"re9CeAC-9EEC","execution_count":288,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0\n"," 1 1 1 1 0 0 1 1 0 1 1 1 1]\n","Score: 0.76\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"]}]},{"cell_type":"markdown","metadata":{"id":"lgB9MQdK9EED"},"source":["You can know compare the various sets of embeddings, and answer the following question:\n","- Why could we expect that the results obtained with embeddings pre-trained with Glove are better than others ? Is that verified ? And what changes if we remove the constraint of having the same 5000 words vocabulary as the other methods ? "],"id":"lgB9MQdK9EED"},{"cell_type":"markdown","metadata":{"id":"XmRSMa6-3Nd8"},"source":["**Answer**: We may expect that the results obtained with Glove embeddings would be better since they are pre-trained, hence having some prior knowledge about the representation. While using the vocabulary of size 5k, it showed the score of 0.76, which is slightly better than the base pipeline (0.74). So the claim is verified. However, after removing the constraint of 5000 words and taking the whole vocabulary, it gave performance similar to base pipeline, i.e. 0.74. <br>\n","In conclusion, it happened so that for this specific case of splitting for train and validation data, there was no improvement with Glove when we used the whole vocabulary. Besides, a high value of k=100 has also an impact on this."],"id":"XmRSMa6-3Nd8"},{"cell_type":"markdown","metadata":{"id":"2Z5BenvH9EED"},"source":["## Fine-tuning of a Bert model\n","\n","Following the idea of the previous lab, fine-tune the lightest Bert model available on IMDB data and compare with the results you obtained just above."],"id":"2Z5BenvH9EED"},{"cell_type":"code","metadata":{"id":"x4-yO8739EED","executionInfo":{"status":"ok","timestamp":1635972861302,"user_tz":-60,"elapsed":192,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["!pip install transformers\n","!pip install fasttext\n","import transformers"],"id":"x4-yO8739EED","execution_count":289,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgbIf7Oe-hHb","executionInfo":{"status":"ok","timestamp":1635972867876,"user_tz":-60,"elapsed":193,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["# For downloading files directly from a Google Drive\n","import gdown\n","\n","import numpy as np\n","import fasttext\n","import torch\n","from transformers import DistilBertTokenizerFast, DistilBertModel, DistilBertForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","\n","from sklearn import neighbors\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","import altair as alt\n","\n","import pprint\n","pp = pprint.PrettyPrinter(indent=4)"],"id":"dgbIf7Oe-hHb","execution_count":290,"outputs":[]},{"cell_type":"code","metadata":{"id":"LO_XpDni9EED","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972871191,"user_tz":-60,"elapsed":1125,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"75226848-67e0-4ad1-c5f6-643a939da5aa"},"source":["tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"],"id":"LO_XpDni9EED","execution_count":291,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.12.3\",\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"8EvGqIX--mXB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972884228,"user_tz":-60,"elapsed":8158,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"a5dd3930-30c5-4d25-ea8c-0c97d959903f"},"source":["model = DistilBertModel.from_pretrained('distilbert-base-uncased')"],"id":"8EvGqIX--mXB","execution_count":292,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.12.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"]}]},{"cell_type":"code","metadata":{"id":"iMQSUtUg-zXj","executionInfo":{"status":"ok","timestamp":1635972885173,"user_tz":-60,"elapsed":304,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["imdb_inputs = tokenizer([ex for ex in train_texts_splt],\n","                         truncation=True,\n","                         padding='max_length', \n","                         return_tensors=\"pt\")\n","imdb_test_inputs = tokenizer([ex for ex in val_texts],\n","                              truncation=True,\n","                              padding='max_length', \n","                              return_tensors=\"pt\")"],"id":"iMQSUtUg-zXj","execution_count":293,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLuyemiHAyBo","executionInfo":{"status":"ok","timestamp":1635972885436,"user_tz":-60,"elapsed":2,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["class FineTuningDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"id":"RLuyemiHAyBo","execution_count":294,"outputs":[]},{"cell_type":"code","metadata":{"id":"5X8moWz3Bt-J","executionInfo":{"status":"ok","timestamp":1635972887365,"user_tz":-60,"elapsed":253,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["imdb_dataset = FineTuningDataset(imdb_inputs, train_labels_splt)\n","imdb_test_dataset = FineTuningDataset(imdb_test_inputs, val_labels)"],"id":"5X8moWz3Bt-J","execution_count":295,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uwm--hgSCC1a","executionInfo":{"status":"ok","timestamp":1635972891071,"user_tz":-60,"elapsed":1324,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"55bf44d0-8b5d-4beb-8608-40cc29c8e262"},"source":["imdb_model_ft = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels= max(train_labels_splt) + 1)"],"id":"Uwm--hgSCC1a","execution_count":296,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.12.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"W5fJtSofCQGd","executionInfo":{"status":"ok","timestamp":1635972892941,"user_tz":-60,"elapsed":5,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  acc = accuracy_score(labels, preds)\n","  return {\n","      'accuracy': acc,\n","  }"],"id":"W5fJtSofCQGd","execution_count":297,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGokPRbJCsJi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635972894403,"user_tz":-60,"elapsed":273,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"ad4611ea-f64c-44e4-f703-294b4ba894db"},"source":["training_args = TrainingArguments(\n","    num_train_epochs=3,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=20,   # batch size for evaluation\n","    learning_rate=5e-5,              # initial learning rate for Adam optimizer\n","    warmup_steps=100,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n","    weight_decay=0.01,               # strength of weight decay\n","    output_dir='./results',          # output directory\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n","    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",")"],"id":"vGokPRbJCsJi","execution_count":298,"outputs":[{"output_type":"stream","name":"stderr","text":["using `logging_steps` to initialize `eval_steps` to 100\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","metadata":{"id":"9ZHRpSWsW0JR","executionInfo":{"status":"ok","timestamp":1635972898568,"user_tz":-60,"elapsed":195,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}}},"source":["imdb_trainer = Trainer(\n","    model=imdb_model_ft,                      \n","    args=training_args,                  \n","    train_dataset=imdb_dataset,         \n","    eval_dataset=imdb_test_dataset,           \n","    compute_metrics=compute_metrics       \n",")"],"id":"9ZHRpSWsW0JR","execution_count":299,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"d6E2PLZAW8g2","executionInfo":{"status":"ok","timestamp":1635974566525,"user_tz":-60,"elapsed":1666988,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"ee4e318c-9a1e-4334-f1dd-0906dbf0a202"},"source":["imdb_trainer.train()"],"id":"d6E2PLZAW8g2","execution_count":300,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 200\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 39\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [39/39 26:58, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=39, training_loss=0.6800793867844802, metrics={'train_runtime': 1666.9842, 'train_samples_per_second': 0.36, 'train_steps_per_second': 0.023, 'total_flos': 79480439193600.0, 'train_loss': 0.6800793867844802, 'epoch': 3.0})"]},"metadata":{},"execution_count":300}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"ZG-V4SHjW-Jj","executionInfo":{"status":"ok","timestamp":1635974624232,"user_tz":-60,"elapsed":44164,"user":{"displayName":"Khalig Aghakarimov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj60wWD6pL4cRrrk1zEQIBKtEozGCXVAe_aGeyevQ=s64","userId":"04231999013299453569"}},"outputId":"67252e70-afbd-4ebb-d2bb-2d220874ff3a"},"source":["imdb_trainer.evaluate()"],"id":"ZG-V4SHjW-Jj","execution_count":302,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 50\n","  Batch size = 20\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:26]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'epoch': 3.0,\n"," 'eval_accuracy': 0.82,\n"," 'eval_loss': 0.6143593788146973,\n"," 'eval_runtime': 43.9893,\n"," 'eval_samples_per_second': 1.137,\n"," 'eval_steps_per_second': 0.068}"]},"metadata":{},"execution_count":302}]},{"cell_type":"markdown","metadata":{"id":"uBD6XvonT2FX"},"source":["**Comparison of BERT model with others**: The BERT model seems to perform better, and in general, the best one among all examined models. It reached an accuracy of *0.82* with 3 epochs which is quite impressive."],"id":"uBD6XvonT2FX"}]}