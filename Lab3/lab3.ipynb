{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgLpzYcqD5Gw"
   },
   "source": [
    "# Introduction to using BERT and application to Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4xhBMfVEPo9"
   },
   "source": [
    "This lab session is heavily inspired from the tutorials by [BERT for Humanists](https://melaniewalsh.github.io/BERT-for-Humanists/tutorials/) and uses data and code from the paper [\"Analysis and Evaluation of Language Models for Word Sense Disambiguation\"](https://arxiv.org/abs/2008.11608) - see the related [github](https://github.com/danlou/bert-disambiguation). All along this session, you should take a look at the paper, try to understand the experimental setup, and try to check if your results are expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg5jRzlJFj5D"
   },
   "source": [
    "We will mainly work with the library ```transformers```, but will also make use of ```fasttext``` and ```sklearn``` to establish baselines. For reference, these results were obtained with versions 4.6.1, 0.9.2 and 0.22.2 respectively. Do not hesitate to use the documentations:\n",
    "- [Transformers](https://huggingface.co/transformers/index.html)\n",
    "- [Fasttext](https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
    "- [Sklearn](https://scikit-learn.org/0.22/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqoVwWbbHBpH"
   },
   "source": [
    "### Prerequisites\n",
    "This lab is intended to be run from colab; if you have your own GPU and Cuda installation, you can download the data separately. If you do not and are very patient, you should be able to run everything by removing the code sending the BERT models, and tensors to be processed by them, to the GPU: ```.to(\"cuda\")```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "a5oY_cKZM45f"
   },
   "outputs": [],
   "source": [
    "# Installation is necessary on the first run\n",
    "\n",
    "# !pip install fasttext\n",
    "# !pip install transformers\n",
    "# !pip install gdown\n",
    "# !pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Lud7KgPgMcqm"
   },
   "outputs": [],
   "source": [
    "# For downloading files directly from a Google Drive\n",
    "import gdown\n",
    "\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QdJuOZmOnD6"
   },
   "outputs": [],
   "source": [
    "# On the first run, download and unzip the data\n",
    "# If you change the path, you will need to change it accordingly in the relevant functions\n",
    "\n",
    "#gdown.download(\"https://docs.google.com/uc?export=download&id=10drbtaUs-qIEQz9Ytkd_VaJmXfZFxU3-\", output=\"data_CoarseWSD-20.zip\", quiet=False)\n",
    "#!unzip /content/data_CoarseWSD-20.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_M906feITdh"
   },
   "source": [
    "### Data\n",
    "From the authors's [github](https://github.com/danlou/bert-disambiguation): the CoarseWSD-20 dataset is a coarse-grained sense disambiguation built from Wikipedia (nouns only) targetting 2 to 5 senses of 20 ambiguous words. It was specifically designed to provide an ideal setting for evaluating WSD models (e.g. no senses in test sets missing from training), both quantitavely and qualitatively.\n",
    "\n",
    "Here, I only use the base dataset - but more versions are available, to experiment on particular aspects of the model (ability to work with fes-shots, or on out-of-domain data).\n",
    "You should take a look at the files to see how they are built. Note that the different senses of the words are explicited in a file; here, to make things easier, we will simply use the number of the classes as labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsvmi33pMcqq"
   },
   "outputs": [],
   "source": [
    "def load_data(word, split, dataset='CoarseWSD-20'):\n",
    "    examples = []\n",
    "    # Open the data file\n",
    "    with open('%s/%s/%s.data.txt' % (dataset, word, split)) as split_data_f:\n",
    "        # Open the corresponding label file\n",
    "        with open('%s/%s/%s.gold.txt' % (dataset, word, split)) as split_gold_f:\n",
    "            # For each line, build an object containing the tokens, the position of the ambiguous word to classify, and the class. \n",
    "            for line, line_n in zip(split_data_f, split_gold_f):\n",
    "                word_idx, tokens = line.split('\\t')\n",
    "                word_idx = int(word_idx)\n",
    "                tokens = tokens.split()\n",
    "                examples.append({'tokens': tokens, 'idx': word_idx, 'class': int(line_n)})\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQEvNUioMcqs"
   },
   "outputs": [],
   "source": [
    "# We will use the first dataset - on apple (the company/the fruit) - as an example \n",
    "apple_train = load_data('apple', 'train', dataset='CoarseWSD-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lqzwms-Mcqt"
   },
   "outputs": [],
   "source": [
    "pp.pprint(len(apple_train))\n",
    "pp.pprint(apple_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xhg8yvlrKglX"
   },
   "source": [
    "### A first baseline: with static word embeddings\n",
    "A first possibility is to use static word embeddings to classify the sentences into senses (which are our classes). We could:\n",
    "- Load pre-trained word embeddings and use them as features, for example by learning a simple classifier on the mean of the embeddings composing the sentence.\n",
    "- Create or load a pre-trained embedding model and fine-tune these embeddings on the classification task.\n",
    "As pre-trained word embeddings are quite heavy to download, we will stick here to creating a *fasttext* model and fine-tuning it, which is quite easy to do with the ```fasttext``` library. \n",
    "The relevant documentation can be found [here](https://fasttext.cc/docs/en/supervised-tutorial.html#getting-and-preparing-the-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RheqSxzXMcqv"
   },
   "outputs": [],
   "source": [
    "apple_test = load_data('apple', 'test', dataset='CoarseWSD-20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSZ9XUs6M1ob"
   },
   "source": [
    "Basically, we just need to create the model, and feed it the data and labels. However, ```fasttext``` requires a specific file format: each line of the text file contains the label, followed by the corresponding sentence. The labels start by the __label__ prefix, which is how fastText recognize what is a label or what is a word. \n",
    "\n",
    "The model is then trained to predict the labels given the words in the sentence.\n",
    "\n",
    "Hence, we need to create files following this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fleHMuLYMcqw"
   },
   "outputs": [],
   "source": [
    "def convert_examples_ft(instance, word, split, dataset='CoarseWSD-20'):\n",
    "    with open('fasttext_data/%s.fasttext.%s.%s' % (dataset, word, split), 'w') as word_split_f:\n",
    "        for example in instance:\n",
    "            inst_str = '__label__%s %s' % (example['class'], ' '.join(example['tokens']))\n",
    "            word_split_f.write('%s\\n' % inst_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_vzJEk2Mcqx"
   },
   "outputs": [],
   "source": [
    "convert_examples_ft(apple_train, 'apple', 'train', dataset='CoarseWSD-20')\n",
    "convert_examples_ft(apple_test, 'apple', 'test', dataset='CoarseWSD-20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lPJ9i83Nf0I"
   },
   "source": [
    "And we can create the models with this function; we can initialize them randomly, or from a set of word representations already trained on a very large corpora. In this second case, training can be long: you should then save the model to re-use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3-upyxZMcqx"
   },
   "outputs": [],
   "source": [
    "def create_model_ft(word, dataset, use_pretrained=False, saving=False):\n",
    "    train_path = 'fasttext_data/%s.fasttext.%s.train' % (dataset, word)\n",
    "    if use_pretrained:\n",
    "        # Create the model from pre-trained vectors; it's pretty slow.\n",
    "        # We use the 'ova' - one-versus-all - loss to deal efficiently with more than 2 labels\n",
    "        model = fasttext.train_supervised(input=train_path,\n",
    "                                          pretrainedVectors='external/fastText-0.9.1/crawl-300d-2M.vec',\n",
    "                                          epoch=25, lr=0.5, dim=300, loss='ova')\n",
    "        if saving:\n",
    "            model_fn = '%s.fasttext.%s.crawl-300d-2M.model.bin' % (dataset, word)\n",
    "\n",
    "    else:\n",
    "        # Creating the model from scratch, we can choose the dimension\n",
    "        model = fasttext.train_supervised(input=train_path,\n",
    "                                          epoch=25, lr=0.5, dim=100, loss='ova')\n",
    "        if saving:\n",
    "            model_fn = '%s.fasttext.%s.100d.model.bin' % (dataset, word)\n",
    "    if saving:\n",
    "        model.save_model('fasttext_models/' + model_fn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVkpiJHUMcqy"
   },
   "outputs": [],
   "source": [
    "apple_f_model = create_model_ft('apple', 'CoarseWSD-20', use_pretrained=False)\n",
    "# The pre-trained vectors are quite heavy (1,4G) and do not contribute much to performance in this case.\n",
    "# apple_f_model_pre = create_model_ft('apple', 'CoarseWSD-20', use_pretrained=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DSmvd1YMcqy"
   },
   "outputs": [],
   "source": [
    "# Let's implement a testing function adapted to the particular data format \n",
    "def test_models(model, word, dataset):\n",
    "    test_path = 'fasttext_data/%s.fasttext.%s.test' % (dataset, word)\n",
    "    # This is a straightforward use of the test function; however, we may want a more detailed look at the perfomances\n",
    "    print(model.test(test_path))\n",
    "    # Manually, let's make the prediction example by example\n",
    "    results = []\n",
    "    with open(test_path) as test_f:\n",
    "        # For each line in the testin file\n",
    "        for line_n, line in enumerate(test_f):\n",
    "            # Cut the line along the whitespaces\n",
    "            elems = line.strip().split()\n",
    "            # The first element is the label: we remove \"__label__\" and keep the last character\n",
    "            # The rest of the list are the words of the sentence\n",
    "            gold_label, tokens = elems[0][-1], elems[1:]\n",
    "    \n",
    "            # The model outputs the labels and their probabilities, in order\n",
    "            labels, probs = model.predict(' '.join(tokens), k=-1)\n",
    "            # Again, remove the \"__label__\" and keep the class number\n",
    "            labels = [int(label[-1]) for label in labels]\n",
    "            # Keep all informations\n",
    "            results.append((int(line_n), tokens, int(gold_label), probs[labels]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLLir0nfMcqz"
   },
   "outputs": [],
   "source": [
    "apple_f_results = test_models(apple_f_model, 'apple', 'CoarseWSD-20')\n",
    "# apple_f_results_pre = test_models(apple_f_model_pre, 'apple', 'CoarseWSD-20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZKgPJTzP5In"
   },
   "source": [
    "We can now take our detailed results to build a confusion matrix, with tools from ```sklearn```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw-_ufQaMcqz"
   },
   "outputs": [],
   "source": [
    "def build_display_cm(results):\n",
    "    true = []\n",
    "    pred = []\n",
    "    # Go across the results to build two lists: one with the true labels,\n",
    "    # the second with the predicted labels\n",
    "    for result in results:\n",
    "        true.append(int(result[2]))\n",
    "        pred.append(np.argmax(result[3]))\n",
    "    # Build the confusion matrix, normalized (or not)\n",
    "    cm = confusion_matrix(true , pred, normalize='true')\n",
    "    # Tool to display it nicely\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(max(true)))\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4awQHY4Mcq0"
   },
   "outputs": [],
   "source": [
    "apple_f_cm = build_display_cm(apple_f_results)\n",
    "# apple_f_cm_pre = build_display_cm(apple_f_results_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfQno9PoQg5A"
   },
   "source": [
    "It works very well ! But maybe the task is easy. Let's create a function to automatize word sense disambiguation experiments, that we can call on any of the ambiguous words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NPd03gGMcq0"
   },
   "outputs": [],
   "source": [
    "def baseline_f(word, dataset, use_pretrained=False, saving=False):\n",
    "    # Complete it ! \n",
    "    return # Return the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxUR4HazMcq1"
   },
   "outputs": [],
   "source": [
    "# Let's try it on one of the more difficult-looking words\n",
    "baseline_f('club', 'CoarseWSD-20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPSAfFCAS7pp"
   },
   "source": [
    "### Encoding data with BERT\n",
    "\n",
    "We're going to transform our data into *tokens* that BERT will then process into embeddings. We will use the tokenizer associated to a BERT-type model, via the ```transformers``` library (from Huggingface).\n",
    "\n",
    "Here are the steps we need to follow:\n",
    "\n",
    "1. The text need to be truncated if it's more than 256/512 tokens or padded if it's less than 256 or 512 tokens (depending on the model size). The words may also need to be separated into subword tokens: BERT-like models use different *tokenizers*, built to handle a fix-sized vocabulary. Any word that is not in the vocabulary is divided into subwords that are. BERT's tokenizer is based on *Wordpiece*. \n",
    "\n",
    "2. BERT uses special tokens, which will be added:\n",
    "    - [CLS] — Start token of every document\n",
    "    - [SEP] — Separator between each sentence \n",
    "    - [PAD] — Padding at the end of the document as many times as necessary, up to 256/512 tokens\n",
    "    - &#35;&#35; — Indicates the start of a subword, or \"word piece\".  \n",
    "\n",
    "We will use the **DistilBERT** model to save space (as it is a distilled version of the full BERT model), and we will use the uncased version. We need to be careful to use the corresponding tokenizer. \n",
    "\n",
    "For each model that you plan to use, you should read the corresponding description in the documention - here for [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvwSVbc5Mcq1"
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4N49q_yMcq2"
   },
   "outputs": [],
   "source": [
    "# Let's use it on an example - here, the 7th data sample \n",
    "# We want tensors to be returned as pytorch tensors ('pt') as we use torch\n",
    "# We give the document as one string, using 'join'\n",
    "print(join(apple_train[6]['tokens']))\n",
    "tokenized_ex = tokenizer(' '.join(apple_train[6]['tokens']), truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4Vo2CE-Mcq2"
   },
   "outputs": [],
   "source": [
    "# Let's look at the output; what about the padding ?\n",
    "pp.pprint(tokenized_ex[0].tokens)\n",
    "pp.pprint(tokenized_ex[0].special_tokens_mask)\n",
    "\n",
    "# The number of tokens is not the same anymore\n",
    "pp.pprint(len(apple_train[6]['tokens']))\n",
    "pp.pprint(len(tokenized_ex[0].tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBSRoX3bMcq2"
   },
   "outputs": [],
   "source": [
    "# We can also give a list of words as input (or even a list of list of words); but we need to use the argument\n",
    "# 'is_split_into_words=True'; this is convenient for some tasks\n",
    "tokenized_ex_split = tokenizer(apple_train[6]['tokens'], truncation=True, padding=True, return_tensors=\"pt\", is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFmEky2RMcq3"
   },
   "outputs": [],
   "source": [
    "pp.pprint(tokenized_ex_split[0].tokens)\n",
    "pp.pprint(len(tokenized_ex_split[0].tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u574u3mhf1AM"
   },
   "source": [
    "### Outputting contextual representations with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9brW1OIMcq3"
   },
   "outputs": [],
   "source": [
    "# We have encoded some data, let's try the model\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixdmOGizMcq3"
   },
   "outputs": [],
   "source": [
    "# We need to send the encoded data to the same device than the model\n",
    "tokenized_ex_split.to(\"cuda\")\n",
    "# The '**' allows to distribute the attributes of the object as arguments of the function\n",
    "output_ex = model(**tokenized_ex_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WfZkC5NMcq3"
   },
   "outputs": [],
   "source": [
    "pp.pprint(output_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sqk4VTBkMcq4"
   },
   "outputs": [],
   "source": [
    "# Let's get the last hidden states as final word representations\n",
    "vectors_ex = output_ex.last_hidden_state[0]\n",
    "# We have 12 tokens, so 12 vectors, of size 768 each\n",
    "pp.pprint(vectors_ex.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvvHvekbXU_v"
   },
   "source": [
    "Now, we would like to process our whole dataset, in order to obtain vectors for each word of each sentence. As BERT provide *contextual* embeddings, meaning that the word representations are functions of contextual words, the BERT embeddings of each occurence of *apple* taken in context should be enough to make a decision on which sense it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hW2osfUMcq4"
   },
   "outputs": [],
   "source": [
    "# Getting all textual data in a list\n",
    "apple_train_texts = [ex['tokens'] for ex in apple_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2VwO8fxMcq4"
   },
   "outputs": [],
   "source": [
    "# Let's build two lists, of tokens, and associated vectors, for the whole dataset\n",
    "apple_token_vectors = []\n",
    "apple_tokens = []\n",
    "# Looping over the examples processing one at a time. By the way, why ? \n",
    "for i, text in enumerate(apple_train_texts):\n",
    "        # Here we tokenize each text with the tokenizer\n",
    "        inputs = tokenizer(' '.join(text), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        # We don't care about the first and last token !\n",
    "        apple_tokens.append(inputs[0].ids[1:-1])\n",
    "        inputs.to(\"cuda\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # When using outputs of the model, we need to send it back to default device to avoid using GPU memory\n",
    "        # We also use .detach() to cut the gradient - which avoids errors and memory issues \n",
    "        apple_token_vectors.append(outputs.last_hidden_state[0,1:-1,:].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zbPGrtmMcq5"
   },
   "outputs": [],
   "source": [
    "# Let's put all vectors in one big array\n",
    "all_apple_token_vectors = np.concatenate(apple_token_vectors, axis=0)\n",
    "pp.pprint(all_apple_token_vectors.shape)\n",
    "\n",
    "# ... and check that we have the right number of tokens\n",
    "all_apple_tokens = np.concatenate(apple_tokens)\n",
    "pp.pprint(len(all_apple_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4EE2HKoMcq5"
   },
   "outputs": [],
   "source": [
    "# Let's try to find the 'apple' tokens\n",
    "search_keyword = \"apple\"\n",
    "word_positions = np.where(all_apple_tokens == tokenizer.vocab[search_keyword])[0]\n",
    "pp.pprint(len(word_positions))\n",
    "pp.pprint(len(apple_train_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28sawKEFZC1N"
   },
   "source": [
    "Now, we are faced with an issue - these supplementary tokens come from the fact that 'apple' may be a wordpiece for another, longer word. \n",
    "As our task is particular, we need to adapt our approach. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu7FMNJyYx7b"
   },
   "source": [
    "The proper and clean way to deal with this is to build a function creating an index mapping between our original tokenization and the wordpiece tokenization used by the BERT model. This mapping may prove useful for other needs ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-vsxFPUMcq6"
   },
   "outputs": [],
   "source": [
    "# We can build a function which creates an index mapping between the original and the wordpiece tokenization.\n",
    "# This way we can retrieve, for each example, the index corresponding to the ambiguous occurrence of 'apple'.\n",
    "\n",
    "# The function will take as input a sentence already split according to our own tokenization\n",
    "# We'll try word by word: if the BERT tokenization divides a word into pieces, we'll keep track of the mapping\n",
    "# We'll get the new tokenization and the mapping between the two as output\n",
    "def tokenize_and_map_idcs(sentence_split, tokenizer, maxlen=512):\n",
    "    '''\n",
    "    :param sentence_split: list of str (words in a sentence)\n",
    "    :param tokenizer: (Distil)BERT tokenizer \n",
    "    :param maxlen: int, maximum sequence length allowed (512 in BERT)\n",
    "    :return tokenized_sentence: list of str\n",
    "    :return map_original_to_bert: list of tuples. For a word at position i in the original sentence (sentence_split),\n",
    "    map_original_to_bert[i] provides the indices that correspond to this word in the newly tokenized sentence.\n",
    "    '''\n",
    "    map_original_to_bert = [] \n",
    "    tokenized_sentence = ['[CLS]'] # Initiating the new tokenized sentence\n",
    "    incomplete = False\n",
    "    for orig_token in sentence_split:\n",
    "        current_tokens_bert_idx = [len(tokenized_sentence)] # Keeping track of the length of the BERT-tokenization at the previous step\n",
    "        bert_token = tokenizer.tokenize(orig_token) # Tokenize the current word (it may result in >1 wordpieces)\n",
    "        # Check if including this token (and the final [SEP] token) will exceed maximum length. If so, stop (truncate here).\n",
    "        if len(tokenized_sentence) + len(bert_token) + 1 > maxlen:\n",
    "            incomplete = True\n",
    "            break\n",
    "        tokenized_sentence.extend(bert_token) # Append the new word piece(s) to the tokenized sentence \n",
    "        if len(bert_token) > 1: # If the current token has been split into multiple wordpieces:\n",
    "            number_extra_pieces = len(bert_token) - 1 # How many additional pieces are there \n",
    "            for _ in range(number_extra_pieces):\n",
    "                current_tokens_bert_idx.append(current_tokens_bert_idx[-1]+1)        \n",
    "        map_original_to_bert.append(tuple(current_tokens_bert_idx)) # Keep track of the mapping\n",
    "\n",
    "    tokenized_sentence.append('[SEP]')\n",
    "\n",
    "    return tokenized_sentence, map_original_to_bert, incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoG7m43QMcq6"
   },
   "outputs": [],
   "source": [
    "# Function to assert that everything checks out\n",
    "def check_correct_token_mapping(bert_tokenized_sentence, bert_positions, target_word, tokenizer):    \n",
    "    tokenized_word = list(tokenizer.tokenize(target_word))\n",
    "    bert_token = [bert_tokenized_sentence[p] for p in bert_positions]    \n",
    "    if bert_token == tokenized_word: \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZ3EhXqOMcq6"
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentence and obtain the mapping\n",
    "tokenized_sentence, map_original_to_bert, incomplete = tokenize_and_map_idcs(apple_train[6]['tokens'], tokenizer)\n",
    "# Retrieve the indexes of \"apple\" in the new tokenization\n",
    "apple_idx = map_original_to_bert[apple_train[6]['idx']]\n",
    "print(type(apple_idx))\n",
    "# Verify that we're right\n",
    "assert check_correct_token_mapping(tokenized_sentence, apple_idx, \"apple\", tokenizer)\n",
    "print(tokenized_sentence)\n",
    "print(map_original_to_bert)\n",
    "print(apple_idx, [tokenized_sentence[i] for i in apple_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sB779FeXMcq7"
   },
   "outputs": [],
   "source": [
    "# Now we can create a function that will only output the vector corresponding to the ambiguous occurence of 'apple' \n",
    "\n",
    "# Note: it is possible that, if a dataset contains very long sentences (>256/512 tokens), \n",
    "# a sentence may be truncated before the occurrence of our target word, so that we can't obtain a vector for it.\n",
    "# This is not the case in this dataset, but a decision should be made for these cases \n",
    "# (e.g., skipping these sentences if they're training instances)\n",
    "\n",
    "\n",
    "def get_vectors_from_bert_model(tokenizer, model, word, examples):\n",
    "    word_vectors = []\n",
    "    \n",
    "    # Go through each example\n",
    "    for i, example in enumerate(examples):\n",
    "        # Get the encoding of the example and the position of the target word (apple) \n",
    "        tokenized_sentence, map_original_to_bert, incomplete = tokenize_and_map_idcs(example['tokens'], tokenizer)\n",
    "        word_position = list(map_original_to_bert[example['idx']])        \n",
    "        assert check_correct_token_mapping(tokenized_sentence, word_position, word, tokenizer)\n",
    "        input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_sentence)])\n",
    "        \n",
    "        inputs = {\"input_ids\": input_ids.to(\"cuda\")}\n",
    "        outputs = model(**inputs)        \n",
    "        word_vectors.append(outputs.last_hidden_state[0,word_position,:].detach().cpu().numpy())        \n",
    "    \n",
    "    # Return the concatenation of these vectors\n",
    "    return np.concatenate(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BU5__KNbnh8"
   },
   "outputs": [],
   "source": [
    "apple_vectors = get_vectors_from_bert_model(tokenizer, model, 'apple', apple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQLiJ-1vMcq8"
   },
   "outputs": [],
   "source": [
    "print(apple_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehlY-1n5duza"
   },
   "source": [
    "### Visualizing representations of ambiguous words\n",
    "\n",
    "Before using these embeddings for disambiguation, we can use a [Singular Value Decomposition](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) to project them in two dimensions, following the two directions of maximal variation. We then put them into a Pandas DataFrame, in order to plot them with the Python data visualization library [Altair](https://altair-viz.github.io/gallery/scatter_tooltips.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDTXFU7mMcq8"
   },
   "outputs": [],
   "source": [
    "# We need to normalize the vectors\n",
    "row_norms = np.sqrt(np.sum(apple_vectors ** 2, axis=1))\n",
    "apple_vectors_norm = apple_vectors / row_norms[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuKTevlOMcq8"
   },
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(apple_vectors_norm)\n",
    "pp.pprint(U.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLaBUZxNMcq8"
   },
   "outputs": [],
   "source": [
    "# Let's use the text as note for each point, and the class as color\n",
    "apple_train_texts_join = [' '.join(ex['tokens']) for ex in apple_train]\n",
    "apple_train_classes = [ex['class'] for ex in apple_train]\n",
    "df = pd.DataFrame({\"x\": U[:,0], \"y\": U[:,1], \"sentence\": apple_train_texts_join, \"class\": apple_train_classes})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EbkIjruMcq9"
   },
   "outputs": [],
   "source": [
    "alt.Chart(df[:]).mark_circle(size=200).encode(\n",
    "    x=\"x\", y=\"y\", color='class',\n",
    "    tooltip=['sentence']\n",
    "    ).interactive().properties(\n",
    "    width=500,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eXeOD_OfEom"
   },
   "source": [
    "In this case, the projections are already very well-separated - we probably will obtain very good results with simple methods. Again, let's create a function to automatize visualization on any ambiguous word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61MXT3ZuMcq-"
   },
   "outputs": [],
   "source": [
    "def get_and_display_bert(word, dataset, tokenizer, model):\n",
    "    # Complete it !\n",
    "    return # Return the chart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dABfxFdJMcq_"
   },
   "outputs": [],
   "source": [
    "club_chart = get_and_display_bert('club', 'CoarseWSD-20', tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NRpG8e5Mcq_"
   },
   "outputs": [],
   "source": [
    "club_chart.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5zIZdgDhk-T"
   },
   "source": [
    "### Looking at the class of the nearest neighbor\n",
    "To compute a distance between two words representation, we will use *cosine similarity*, which measures the angle between vectors but ignores their length. We will then use a ```sklearn``` model to find the nearest neighbor of ambiguous word representations among the training examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXmhzepgmY-s"
   },
   "outputs": [],
   "source": [
    "apple_test = load_data('apple', 'test', dataset='CoarseWSD-20')\n",
    "apple_test_vectors = get_vectors_from_bert_model(tokenizer, model, 'apple', apple_test)\n",
    "row_norms = np.sqrt(np.sum(apple_test_vectors ** 2, axis=1))\n",
    "apple_test_vectors_norm = apple_test_vectors / row_norms[:,np.newaxis]\n",
    "apple_test_classes = [ex['class'] for ex in apple_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-9in7VimZhd"
   },
   "outputs": [],
   "source": [
    "apple_nn_model = neighbors.KNeighborsClassifier(1, metric='cosine').fit(apple_vectors_norm, apple_train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gzNSEjTmZv1"
   },
   "outputs": [],
   "source": [
    "pred = apple_nn_model.predict(apple_test_vectors_norm)\n",
    "true = apple_test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vdWiXK_mZ-d"
   },
   "outputs": [],
   "source": [
    "print(classification_report(true, pred))\n",
    "cm = confusion_matrix(true , pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(max(true)))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFBmRHGNqsp8"
   },
   "outputs": [],
   "source": [
    "def baseline_nn(word, dataset, tokenizer, model):\n",
    "    # Complete it !\n",
    "    # Display the confusion matrix from the function\n",
    "    return # Return the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH-rCQkWtj37"
   },
   "outputs": [],
   "source": [
    "baseline_nn('club', 'CoarseWSD-20', tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjmI0jsTihiP"
   },
   "source": [
    "### A little better: using a logistic regression\n",
    "Following a similar process, we use a logistic regression classifier from ```sklearn``` that we train on ambiguous word representations from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBmSqHPiMcrA"
   },
   "outputs": [],
   "source": [
    "apple_b_model = LogisticRegression(max_iter=1000).fit(apple_vectors, apple_train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c98pyqxHb1UB"
   },
   "outputs": [],
   "source": [
    "pred = apple_b_model.predict(apple_test_vectors)\n",
    "true = apple_test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDQwmcORcH_2"
   },
   "outputs": [],
   "source": [
    "print(classification_report(true, pred))\n",
    "cm = confusion_matrix(true , pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(max(true)))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDAXXH1aeZbt"
   },
   "outputs": [],
   "source": [
    "def baseline_b(word, dataset, tokenizer, model):\n",
    "    # Complete it !\n",
    "    # Display the confusion matrix from the function\n",
    "    return # Return the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1n935YGfGO7"
   },
   "outputs": [],
   "source": [
    "baseline_b('club', 'CoarseWSD-20', tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghOiPIUqjBX4"
   },
   "source": [
    "### Finally: fine-tuning a BERT model \n",
    "The ```transformer``` library provides classes specifically made for fine-tuning BERT models for many tasks, among themselves sequence and token classification. To make things simpler, since have one label by sequence, we will consider that our task is to classify the full sequence (and not the ambiguous word *taken in context* as before). We could probably stick to our previous setting by adapting it into a token classification task, but that may reveal difficult to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_3jW2oO3Aud"
   },
   "outputs": [],
   "source": [
    "# First, encode training and testing data. Note that we do it for the full dataset at once, this time.\n",
    "apple_inputs = tokenizer([' '.join(ex['tokens']) for ex in apple_train],\n",
    "                         truncation=True,\n",
    "                         padding='max_length', \n",
    "                         return_tensors=\"pt\")\n",
    "apple_test_inputs = tokenizer([' '.join(ex['tokens']) for ex in apple_test],\n",
    "                              truncation=True,\n",
    "                              padding='max_length', \n",
    "                              return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KY2jgvr9lNJs"
   },
   "source": [
    "We need to create a custom Torch class to transform encodings into inputs to the model, for two main reasons:\n",
    "- Adding the labels to the encoding object, since it is required for fine-tuning a classification model.\n",
    "- Providing an interface for the model to make mini-batches out of the encodings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQVqbT4CuNvN"
   },
   "outputs": [],
   "source": [
    "class FineTuningDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM6-yMQa8wyz"
   },
   "outputs": [],
   "source": [
    "apple_dataset = FineTuningDataset(apple_inputs, apple_train_classes)\n",
    "apple_test_dataset = FineTuningDataset(apple_test_inputs, apple_test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbmVjeLj1FR6"
   },
   "outputs": [],
   "source": [
    "apple_model_ft = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels= max(apple_train_classes) + 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPgWTRqh2ED8"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzJwO7rH1RsT"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
    "    learning_rate=5e-5,              # initial learning rate for Adam optimizer\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    output_dir='./results',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwNs60qo12CS"
   },
   "outputs": [],
   "source": [
    "apple_trainer = Trainer(\n",
    "    model=apple_model_ft,                      \n",
    "    args=training_args,                  \n",
    "    train_dataset=apple_dataset,         \n",
    "    eval_dataset=apple_test_dataset,           \n",
    "    compute_metrics=compute_metrics       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C5iZj7Y2Nby"
   },
   "outputs": [],
   "source": [
    "apple_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2t960ZfcCwRH"
   },
   "outputs": [],
   "source": [
    "apple_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIjmhduR2TtP"
   },
   "outputs": [],
   "source": [
    "def fine_tuning(word, dataset, tokenizer, training_args, compute_metrics):\n",
    "    # Complete it !\n",
    "    # Display the confusion matrix from the function\n",
    "    return # Return the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwTUJJpD7sxk"
   },
   "outputs": [],
   "source": [
    "club_ft_model = fine_tuning('club', 'CoarseWSD-20', tokenizer, training_args, compute_metrics)\n",
    "club_ft_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzfejdURC5D4"
   },
   "outputs": [],
   "source": [
    "club_ft_model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_for_disambiguation_students.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
